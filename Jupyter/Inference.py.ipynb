{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Section 1: Import Required Libraries and Modules**\n",
    "\n",
    "In this section, we set up the essential libraries needed for data storage, retrieval, and processing in our NLP pipeline. Each library and module plays a specific role:\n",
    "\n",
    "\n",
    "\n",
    "This import setup establishes a comprehensive environment for NLP tasks, data management, and seamless integration with external sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  Milvus manages vector-based storage and retrieval, supported by `connections`, `utility`, and `Collection` for operations like connecting, schema creation, and collection management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Milvus utilities for connections, schema creation, and managing collections.\n",
    "from pymilvus import connections, utility, Collection, CollectionSchema, FieldSchema, DataType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  LangChain Community Embeddings: Utilizes `HuggingFaceEmbeddings` to convert text data into vector embeddings for efficient processing and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import HuggingFace embeddings to convert text into vector representations.\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings  # Adjust if necessary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain Core Utilities: Includes `StrOutputParser` for formatting model outputs as strings, `PromptTemplate` for structuring prompts, and `RunnablePassthrough` for passing data through steps without modifications.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import parser to ensure the model outputs are formatted as strings.\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Import PromptTemplate to structure and format prompts for the language model.\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Import RunnablePassthrough to allow passing data unchanged through multiple steps.\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Import Milvus to interact with the vector database for storage and retrieval.\n",
    "from langchain_milvus import Milvus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Milvus Hybrid Search Retriever: Utilizes `MilvusCollectionHybridSearchRetriever` and `BM25SparseEmbedding` to combine dense and sparse search capabilities for optimized retrieval.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import hybrid search retriever from Milvus, combining dense and sparse search techniques.\n",
    "from langchain_milvus.retrievers import MilvusCollectionHybridSearchRetriever\n",
    "\n",
    "# Import BM25 for sparse embedding, typically used for keyword-based search.\n",
    "from langchain_milvus.utils.sparse import BM25SparseEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatMistralAI offers a conversational interface with Mistralâ€™s language model for interactive querying.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ChatMistralAI to enable conversational interaction with the Mistral AI model.\n",
    "from langchain_mistralai.chat_models import ChatMistralAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web Scraping Tools: Use `requests` for HTTP requests and `BeautifulSoup` to parse HTML content for data extraction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import BeautifulSoup to parse and extract text content from HTML web pages.\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Import requests to make HTTP requests for web scraping or API calls.\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Environment and Utility Modules: `nltk` tokenizes text for NLP, `os` and `dotenv` manage environment variables securely, and `urljoin` joins URLs for web scraping.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import nltk (Natural Language Toolkit) for tokenizing text into sentences.\n",
    "import nltk\n",
    "\n",
    "# Import os for interacting with the operating system, such as handling environment variables.\n",
    "import os\n",
    "\n",
    "# Import urljoin to join relative and absolute URLs for web scraping.\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Import load_dotenv to load environment variables from a .env file into the program.\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Section 2: Load Environment Variables and Setup NLTK**\n",
    "\n",
    "This section configures secure access and text processing. Environment variables are loaded from a `.env` file using `load_dotenv()`, and the `punkt` tokenizer from NLTK is downloaded for sentence-level tokenization in NLP tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load environment variables from a .env file to access configuration settings, such as API keys.\n",
    "load_dotenv()\n",
    "\n",
    "# Download the 'punkt' tokenizer, which allows nltk to split text into sentences.\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Section 3: Configure Mistral API Key**\n",
    "\n",
    "This section highlights the importance of securely managing the `MISTRAL_API_KEY` by using environment variables. The API key is retrieved using `os.getenv()` and set as an environment variable with `os.environ`, ensuring secure and seamless access to Mistral services without hardcoding sensitive information in the code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve the Mistral API key from the environment variables using os.getenv().\n",
    "mistral_api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "\n",
    "# Set the Mistral API key as an environment variable so it can be accessed throughout the code.\n",
    "os.environ[\"MISTRAL_API_KEY\"] = mistral_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Section 4: Connect to Milvus and Setup Collection Schema**\n",
    "\n",
    "In this section, we connect to the Milvus database on localhost at port 19530, ensuring our application can communicate with Milvus for data operations. We then check for an existing collection named `my_collection`, dropping it if necessary to avoid duplicates. Next, we define the collection schema using `FieldSchema` objects, including an `id` field of type `INT64` as the primary key and an `embedding` field of type `FLOAT_VECTOR` with a dimensionality of `768`. Finally, we create the collection in Milvus to efficiently store and retrieve vector embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Establish a connection to Milvus, using localhost as the host and default port 19530.\n",
    "connections.connect(alias=\"default\", host=\"localhost\", port=\"19530\")\n",
    "\n",
    "# Check if the collection \"my_collection\" already exists in Milvus.\n",
    "if utility.has_collection(\"my_collection\"):\n",
    "    # If the collection exists, drop it to avoid duplicate entries.\n",
    "    utility.drop_collection(\"my_collection\")\n",
    "\n",
    "# Define fields for the Milvus collection schema. The ID field acts as the primary key.\n",
    "fields = [\n",
    "    FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True),  # Integer primary key field.\n",
    "    FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=768)  # 768-dimensional float vector field.\n",
    "]\n",
    "\n",
    "# Create a collection schema using the defined fields.\n",
    "schema = CollectionSchema(fields, description=\"My Collection for Embeddings\")\n",
    "\n",
    "# Initialize the collection with the name \"my_collection\" and the created schema.\n",
    "collection = Collection(name=\"my_collection\", schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Section 5: Generate Embeddings and Insert Data into Milvus**\n",
    "\n",
    " This section outlines generating vector embeddings for textual data and inserting them into the Milvus collection. We initialize the Hugging Face embeddings model `sentence-transformers/all-MiniLM-L6-v2` to convert text into dense vector embeddings. Sample data with IDs `[1, 2, 3]` are defined, and embeddings are generated for the sentences \"Hello world,\" \"How are you?,\" and \"Goodbye\" using the `embed_documents` method. Finally, the embeddings and their IDs are inserted into the Milvus collection for efficient storage and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the HuggingFace embeddings model to generate vector representations from text.\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Define some sample data to insert into the Milvus collection.\n",
    "ids = [1, 2, 3]  # List of IDs for the data points.\n",
    "\n",
    "# Generate embeddings for three sample sentences.\n",
    "vectors = embeddings.embed_documents([\"Hello world\", \"How are you?\", \"Goodbye\"])\n",
    "\n",
    "# Insert the IDs and corresponding vector embeddings into the Milvus collection.\n",
    "collection.insert([ids, vectors])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Section 6: Load the Mistral Chat Model**\n",
    "\n",
    "In this section, we load the `ChatMistralAI` model to enable conversational response generation. By passing the Mistral API key for authorization, we ensure secure access to the modelâ€™s features for natural language understanding and response generation in applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load the ChatMistralAI model using the Mistral API key for authorization.\n",
    "chat_model = ChatMistralAI(api_key=mistral_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Section 7: Configure the Milvus Hybrid Search Retriever**\n",
    "\n",
    "In this section, we establish a hybrid search retriever to combine dense and sparse retrieval methods for better search results in the Milvus collection. Utilizing dense embeddings from the Hugging Face model and sparse embeddings with the BM25 algorithm, we enable efficient searches by specifying the previously created Milvus collection. This setup enhances semantic and keyword-based searches and configures the retriever to return the top 2 most relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Set up a hybrid search retriever using the Milvus collection and various embeddings.\n",
    "retriever = MilvusCollectionHybridSearchRetriever(\n",
    "    collection=collection,  # Specify the Milvus collection to search.\n",
    "    dense_embedding=embeddings,  # Use HuggingFace dense embeddings for semantic search.\n",
    "    sparse_embedding=BM25SparseEmbedding(),  # Use BM25 for sparse, keyword-based search.\n",
    "    top_k=2  # Return the top 2 most relevant documents.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Section 8: Create a Prompt Template and Output Parser**\n",
    "\n",
    "This section involves creating a `PromptTemplate` to structure user queries and an output parser to format the model's responses. The template will use the `question` input variable, formatted as \"Answer the following question: {question}\", ensuring clarity and accuracy in model responses. An output parser will convert responses to strings for consistent and easy use.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define a prompt template to structure user queries for the language model.\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],  # Specify the expected input variable.\n",
    "    template=\"Answer the following question: {question}\"  # Format of the prompt.\n",
    ")\n",
    "\n",
    "# Create an output parser to ensure the model response is converted to a string.\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Section 9: Set Up a RunnablePassthrough**\n",
    "\n",
    "This section sets up a `RunnablePassthrough`, a utility that passes data unchanged through workflow steps, maintaining data integrity while enabling flexible processing. It acts as a pass-through mechanism, allowing data to flow through different pipeline components without alterations. This is beneficial for integrating various processing steps without modifying input data, serving as a placeholder where transformations are conditionally applied.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create a passthrough to pass data unchanged through different steps of the workflow.\n",
    "passthrough = RunnablePassthrough()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Section 10: Define a Web Scraping Function**\n",
    "\n",
    "  This section defines a function, `scrape_website(url)`, to extract text content from a website URL for further analysis. Using `requests` for HTTP GET requests and `BeautifulSoup` for HTML parsing, the function retrieves webpage content. It then extracts and joins visible text using `soup.stripped_strings`, and tokenizes the text into sentences with NLTK's `sent_tokenize`. The function returns a list of sentences, useful for NLP or text analysis.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define a function to scrape text from a given website URL.\n",
    "def scrape_website(url):\n",
    "    # Send an HTTP GET request to the specified URL.\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup.\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extract and join all visible text from the HTML.\n",
    "    text = ' '.join(soup.stripped_strings)\n",
    "\n",
    "    # Tokenize the extracted text into individual sentences using nltk.\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "    # Return the list of sentences extracted from the webpage.\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Section 11: Scrape a Website and Insert Data into Milvus**\n",
    "\n",
    "In this section, we scrape text data from a specified website, generate embeddings for the extracted sentences, and insert these embeddings into the Milvus collection. Using the placeholder URL `\"https://example.com\"`, we extract sentences with the `scrape_website(url)` function, generating embeddings via the Hugging Face `embed_documents` method. Unique IDs for each sentence are created with `list(range(len(sentences)))` and the `insert` method of Milvus stores the sentences and embeddings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define a website URL for scraping (example URL).\n",
    "website_url = \"https://example.com\"\n",
    "\n",
    "# Scrape the website to get a list of sentences.\n",
    "sentences = scrape_website(website_url)\n",
    "\n",
    "# Generate embeddings for the scraped sentences.\n",
    "sentence_embeddings = embeddings.embed_documents(sentences)\n",
    "\n",
    "# Insert the sentences and their embeddings into the Milvus collection with autogenerated IDs.\n",
    "collection.insert([list(range(len(sentences))), sentence_embeddings])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Section 12: Perform a Hybrid Search and Display Results**\n",
    "\n",
    "This section focuses on performing a hybrid search on the Milvus collection using a user-defined query to retrieve and display the most relevant documents. We begin by defining a user query, such as `\"What is this website about?\"`. Using the hybrid search retriever and the `get_relevant_documents(query)` method, we leverage dense and sparse embeddings to find semantically and contextually relevant documents. Finally, we display the content of these retrieved documents, providing insights based on the information available in the Milvus collection.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define a user query to search the collection.\n",
    "query = \"What is this website about?\"\n",
    "\n",
    "# Use the retriever to get the most relevant documents based on the query.\n",
    "results = retriever.get_relevant_documents(query)\n",
    "\n",
    "# Iterate through the retrieved results and print their content.\n",
    "for result in results:\n",
    "    print(result.page_content)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
