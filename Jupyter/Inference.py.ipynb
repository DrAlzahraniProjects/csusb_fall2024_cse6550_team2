{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Section 1: Import Required Libraries and Modules**\n",
    "\n",
    "In this section, we set up the essential libraries needed for data storage, retrieval, and processing in our NLP pipeline. Each library and module plays a specific role:\n",
    "\n",
    "- **Milvus**: Manages vector-based storage and retrieval.\n",
    "  - `connections`, `utility`, `Collection`, etc., support Milvus operations like connecting, creating schemas, and managing collections.\n",
    "  \n",
    "- **LangChain Community Embeddings**: \n",
    "  - `HuggingFaceEmbeddings`: Converts text data into vector embeddings for efficient processing and retrieval.\n",
    "  \n",
    "- **LangChain Core Utilities**:\n",
    "  - `StrOutputParser`: Formats the output of language models as strings.\n",
    "  - `PromptTemplate`: Structures prompts for consistent and accurate model responses.\n",
    "  - `RunnablePassthrough`: Passes data through various steps without modifications, supporting flexible workflows.\n",
    "\n",
    "- **Milvus Hybrid Search Retriever**:\n",
    "  - `MilvusCollectionHybridSearchRetriever` and `BM25SparseEmbedding`: Combines dense and sparse search capabilities for optimized retrieval.\n",
    "\n",
    "- **ChatMistralAI**:\n",
    "  - Provides a conversational interface with Mistral’s language model for interactive querying.\n",
    "\n",
    "- **Web Scraping Tools**:\n",
    "  - `requests` and `BeautifulSoup`: Make HTTP requests and parse HTML content for extracting data from web pages.\n",
    "\n",
    "- **Environment and Utility Modules**:\n",
    "  - `nltk`: Tokenizes text, preparing it for NLP processing.\n",
    "  - `os` and `dotenv`: Manage environment variables for secure configurations.\n",
    "  - `urljoin`: Joins URLs for dynamic web scraping.\n",
    "\n",
    "This import setup establishes a comprehensive environment for NLP tasks, data management, and seamless integration with external sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import Milvus utilities for connections, schema creation, and managing collections.\n",
    "from pymilvus import connections, utility, Collection, CollectionSchema, FieldSchema, DataType\n",
    "\n",
    "# Import HuggingFace embeddings to convert text into vector representations.\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings  # Adjust if necessary.\n",
    "\n",
    "# Import parser to ensure the model outputs are formatted as strings.\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Import PromptTemplate to structure and format prompts for the language model.\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Import RunnablePassthrough to allow passing data unchanged through multiple steps.\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Import Milvus to interact with the vector database for storage and retrieval.\n",
    "from langchain_milvus import Milvus\n",
    "\n",
    "# Import hybrid search retriever from Milvus, combining dense and sparse search techniques.\n",
    "from langchain_milvus.retrievers import MilvusCollectionHybridSearchRetriever\n",
    "\n",
    "# Import BM25 for sparse embedding, typically used for keyword-based search.\n",
    "from langchain_milvus.utils.sparse import BM25SparseEmbedding\n",
    "\n",
    "# Import ChatMistralAI to enable conversational interaction with the Mistral AI model.\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "# Import BeautifulSoup to parse and extract text content from HTML web pages.\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Import requests to make HTTP requests for web scraping or API calls.\n",
    "import requests\n",
    "\n",
    "# Import nltk (Natural Language Toolkit) for tokenizing text into sentences.\n",
    "import nltk\n",
    "\n",
    "# Import os for interacting with the operating system, such as handling environment variables.\n",
    "import os\n",
    "\n",
    "# Import urljoin to join relative and absolute URLs for web scraping.\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Import load_dotenv to load environment variables from a .env file into the program.\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Section 2: Load Environment Variables and Setup NLTK**\n",
    "\n",
    "This section sets up essential configurations for secure access and text processing:\n",
    "\n",
    "- **Environment Variables**: Loads configuration settings, like API keys, from a `.env` file using `load_dotenv()`.\n",
    "\n",
    "- **NLTK Tokenizer**: Downloads the `punkt` tokenizer to enable sentence-level tokenization, which is useful for processing text data in NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load environment variables from a .env file to access configuration settings, such as API keys.\n",
    "load_dotenv()\n",
    "\n",
    "# Download the 'punkt' tokenizer, which allows nltk to split text into sentences.\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Section 3: Configure Mistral API Key**\n",
    "\n",
    "This section focuses on configuring the Mistral API key to ensure secure access to the Mistral services. Proper management of API keys is crucial in any application to prevent unauthorized access and to maintain security.\n",
    "\n",
    "- **Environment Variables**: \n",
    "  - The Mistral API key is retrieved from the environment variables. This is accomplished using the `os.getenv()` method, which fetches the value associated with the `MISTRAL_API_KEY` environment variable. Storing sensitive information like API keys in environment variables helps to keep them secure and out of the source code.\n",
    "  \n",
    "- **Setting the API Key**: \n",
    "  - Once retrieved, the API key is set as an environment variable using `os.environ`. This makes the API key accessible throughout the code, allowing seamless interaction with Mistral services without hardcoding the key in the codebase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve the Mistral API key from the environment variables using os.getenv().\n",
    "mistral_api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "\n",
    "# Set the Mistral API key as an environment variable so it can be accessed throughout the code.\n",
    "os.environ[\"MISTRAL_API_KEY\"] = mistral_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Section 4: Connect to Milvus and Setup Collection Schema**\n",
    "\n",
    "In this section, we will establish a connection to the Milvus database and define the schema for a collection that will store embeddings. Milvus is a vector database designed for efficient similarity search and analytics of unstructured data.\n",
    "\n",
    "- **Establish Connection**: \n",
    "  - We will connect to the Milvus server running on `localhost` at the default port `19530`. This allows our application to communicate with the Milvus instance for data operations.\n",
    "\n",
    "- **Check for Existing Collection**: \n",
    "  - Before creating a new collection, we check if a collection named `my_collection` already exists. If it does, we drop the existing collection to avoid duplicate entries and ensure a clean slate for data insertion.\n",
    "\n",
    "- **Define Collection Schema**: \n",
    "  - The schema for the collection is defined using `FieldSchema` objects. The schema includes:\n",
    "    - An `id` field of type `INT64`, which serves as the primary key for the collection.\n",
    "    - An `embedding` field of type `FLOAT_VECTOR`, with a dimensionality of `768`, which is typically used for storing embeddings from models like BERT.\n",
    "\n",
    "- **Create the Collection**: \n",
    "  - Finally, we create the collection in Milvus with the defined schema, allowing us to store and retrieve vector embeddings efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Establish a connection to Milvus, using localhost as the host and default port 19530.\n",
    "connections.connect(alias=\"default\", host=\"localhost\", port=\"19530\")\n",
    "\n",
    "# Check if the collection \"my_collection\" already exists in Milvus.\n",
    "if utility.has_collection(\"my_collection\"):\n",
    "    # If the collection exists, drop it to avoid duplicate entries.\n",
    "    utility.drop_collection(\"my_collection\")\n",
    "\n",
    "# Define fields for the Milvus collection schema. The ID field acts as the primary key.\n",
    "fields = [\n",
    "    FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True),  # Integer primary key field.\n",
    "    FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=768)  # 768-dimensional float vector field.\n",
    "]\n",
    "\n",
    "# Create a collection schema using the defined fields.\n",
    "schema = CollectionSchema(fields, description=\"My Collection for Embeddings\")\n",
    "\n",
    "# Initialize the collection with the name \"my_collection\" and the created schema.\n",
    "collection = Collection(name=\"my_collection\", schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Section 5: Generate Embeddings and Insert Data into Milvus**\n",
    "\n",
    "In this section, we will generate vector embeddings for textual data and insert these embeddings into the Milvus collection we created in the previous section. This allows for efficient storage and retrieval of high-dimensional data.\n",
    "\n",
    "- **Initialize the Embeddings Model**: \n",
    "  - We will use the Hugging Face embeddings model `sentence-transformers/all-MiniLM-L6-v2`, which is optimized for generating vector representations from text. This model converts sentences into dense vector embeddings that can be easily compared and analyzed.\n",
    "\n",
    "- **Define Sample Data**: \n",
    "  - For demonstration purposes, we define a list of sample IDs corresponding to the text data we want to embed. In this example, we have three IDs: `[1, 2, 3]`.\n",
    "\n",
    "- **Generate Embeddings**: \n",
    "  - We generate embeddings for three sample sentences: \"Hello world\", \"How are you?\", and \"Goodbye\". The `embed_documents` method of the embeddings model is used to produce the corresponding vector representations.\n",
    "\n",
    "- **Insert Data into Milvus**: \n",
    "  - Finally, we insert the generated embeddings along with their corresponding IDs into the Milvus collection. This step ensures that the embeddings are stored and can be accessed for future retrieval and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the HuggingFace embeddings model to generate vector representations from text.\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Define some sample data to insert into the Milvus collection.\n",
    "ids = [1, 2, 3]  # List of IDs for the data points.\n",
    "\n",
    "# Generate embeddings for three sample sentences.\n",
    "vectors = embeddings.embed_documents([\"Hello world\", \"How are you?\", \"Goodbye\"])\n",
    "\n",
    "# Insert the IDs and corresponding vector embeddings into the Milvus collection.\n",
    "collection.insert([ids, vectors])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Section 6: Load the Mistral Chat Model**\n",
    "\n",
    "In this section, we will load the Mistral Chat model, which allows us to utilize its capabilities for generating responses in a conversational format. This model can be integrated into applications that require natural language understanding and response generation.\n",
    "\n",
    "- **Load the Chat Model**: \n",
    "  - We will instantiate the `ChatMistralAI` model, passing the Mistral API key for authorization. This key is essential for authenticating requests made to the Mistral service, ensuring that only authorized applications can access the model's features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load the ChatMistralAI model using the Mistral API key for authorization.\n",
    "chat_model = ChatMistralAI(api_key=mistral_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Section 7: Configure the Milvus Hybrid Search Retriever**\n",
    "\n",
    "In this section, we will set up a hybrid search retriever that combines the strengths of both dense and sparse retrieval methods to enhance search capabilities in the Milvus collection. This hybrid approach allows for more comprehensive and accurate search results.\n",
    "\n",
    "- **Hybrid Search Retriever**: \n",
    "  - The hybrid search retriever integrates different retrieval techniques, enabling efficient searching across various types of embeddings. In this case, we will utilize both dense embeddings generated from the Hugging Face model and sparse embeddings based on the BM25 algorithm, which is effective for keyword-based searches.\n",
    "\n",
    "- **Milvus Collection**: \n",
    "  - We will specify the Milvus collection that we previously created. This collection will serve as the source of the data being searched.\n",
    "\n",
    "- **Embeddings**: \n",
    "  - We will use the Hugging Face embeddings for semantic search, allowing us to retrieve documents based on the meaning and context of the queries.\n",
    "\n",
    "- **BM25 Sparse Embedding**: \n",
    "  - For keyword-based search, we will implement the BM25 algorithm, which ranks documents based on term frequency and document frequency. This enhances the ability to retrieve relevant documents based on exact keyword matches.\n",
    "\n",
    "- **Top K Results**: \n",
    "  - We will configure the retriever to return the top 2 most relevant documents based on the hybrid search criteria.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Set up a hybrid search retriever using the Milvus collection and various embeddings.\n",
    "retriever = MilvusCollectionHybridSearchRetriever(\n",
    "    collection=collection,  # Specify the Milvus collection to search.\n",
    "    dense_embedding=embeddings,  # Use HuggingFace dense embeddings for semantic search.\n",
    "    sparse_embedding=BM25SparseEmbedding(),  # Use BM25 for sparse, keyword-based search.\n",
    "    top_k=2  # Return the top 2 most relevant documents.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Section 8: Create a Prompt Template and Output Parser**\n",
    "\n",
    "In this section, we will create a prompt template that structures user queries for the language model and establish an output parser to format the model's responses. This ensures that the input to the model is well-defined and that the output is in the desired format.\n",
    "\n",
    "- **Prompt Template**: \n",
    "  - We will define a `PromptTemplate` to structure user queries. This template specifies the expected input variables and the overall format of the prompt. By organizing the input in a consistent manner, we enhance the model's understanding and response accuracy.\n",
    "\n",
    "- **Input Variables**: \n",
    "  - The template will include an input variable called `question`, which represents the user’s query that we want the model to answer. \n",
    "\n",
    "- **Template Structure**: \n",
    "  - The prompt will be formatted as: \"Answer the following question: {question}\". This clear and straightforward structure helps guide the model in generating relevant responses.\n",
    "\n",
    "- **Output Parser**: \n",
    "  - An output parser is created to ensure that the model’s responses are appropriately converted to strings. This is crucial for maintaining consistency and ease of use in further processing or display.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define a prompt template to structure user queries for the language model.\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],  # Specify the expected input variable.\n",
    "    template=\"Answer the following question: {question}\"  # Format of the prompt.\n",
    ")\n",
    "\n",
    "# Create an output parser to ensure the model response is converted to a string.\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Section 9: Set Up a RunnablePassthrough**\n",
    "\n",
    "In this section, we will set up a `RunnablePassthrough`, which is a utility that allows data to be passed unchanged through various steps of a workflow. This is useful for scenarios where we need to maintain the integrity of the data while still allowing for the flexibility of the processing pipeline.\n",
    "\n",
    "- **RunnablePassthrough**: \n",
    "  - The `RunnablePassthrough` acts as a simple pass-through mechanism in the data processing workflow. It allows the data to flow through different components of the pipeline without any alterations.\n",
    "\n",
    "- **Use Cases**: \n",
    "  - This can be particularly beneficial when you want to integrate different processing steps without modifying the input data. For instance, it can serve as a placeholder in a workflow where certain transformations are conditionally applied.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create a passthrough to pass data unchanged through different steps of the workflow.\n",
    "passthrough = RunnablePassthrough()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Section 10: Define a Web Scraping Function**\n",
    "\n",
    "In this section, we will define a function that scrapes text content from a specified website URL. This function will be useful for gathering textual data from the web for further processing or analysis.\n",
    "\n",
    "- **Web Scraping Function**: \n",
    "  - The function `scrape_website(url)` is designed to retrieve and extract text from a given webpage. It utilizes libraries like `requests` for making HTTP requests and `BeautifulSoup` for parsing HTML content.\n",
    "\n",
    "- **HTTP GET Request**: \n",
    "  - The function begins by sending an HTTP GET request to the specified URL using `requests.get(url)`. This retrieves the content of the webpage.\n",
    "\n",
    "- **HTML Parsing**: \n",
    "  - The HTML content is then parsed with BeautifulSoup. This library simplifies the process of navigating and manipulating HTML documents, making it easy to extract relevant information.\n",
    "\n",
    "- **Text Extraction**: \n",
    "  - We extract and join all visible text from the HTML using the `soup.stripped_strings` generator, which collects all text elements while stripping unnecessary whitespace.\n",
    "\n",
    "- **Tokenization**: \n",
    "  - The extracted text is further processed by tokenizing it into individual sentences using NLTK's `sent_tokenize` function. This step is essential for breaking down the text into manageable pieces for analysis.\n",
    "\n",
    "- **Return Value**: \n",
    "  - Finally, the function returns a list of sentences extracted from the webpage, which can be used for various applications such as natural language processing (NLP) or text analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define a function to scrape text from a given website URL.\n",
    "def scrape_website(url):\n",
    "    # Send an HTTP GET request to the specified URL.\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup.\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extract and join all visible text from the HTML.\n",
    "    text = ' '.join(soup.stripped_strings)\n",
    "\n",
    "    # Tokenize the extracted text into individual sentences using nltk.\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "    # Return the list of sentences extracted from the webpage.\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Section 11: Scrape a Website and Insert Data into Milvus**\n",
    "\n",
    "In this section, we will scrape text data from a specified website, generate embeddings for the extracted sentences, and insert these embeddings into the Milvus collection. This process allows us to enrich the Milvus database with real-time data collected from the web.\n",
    "\n",
    "- **Website URL for Scraping**: \n",
    "  - We start by defining the URL of the website we want to scrape. For this example, we use a placeholder URL (`\"https://example.com\"`), which you can replace with any valid website.\n",
    "\n",
    "- **Scraping Sentences**: \n",
    "  - We will use the previously defined `scrape_website(url)` function to extract sentences from the specified webpage. This function retrieves the content, parses the HTML, and tokenizes the text into individual sentences.\n",
    "\n",
    "- **Generating Embeddings**: \n",
    "  - Once we have the list of sentences, we generate embeddings for these sentences using the `embed_documents` method from the Hugging Face embeddings model. This step transforms the sentences into numerical vector representations suitable for storage and retrieval.\n",
    "\n",
    "- **Inserting Data into Milvus**: \n",
    "  - Finally, we insert the sentences along with their embeddings into the Milvus collection. We generate unique IDs for each sentence using `list(range(len(sentences)))`, ensuring each entry is identifiable. The `insert` method of the collection is then used to store the sentences and their corresponding embeddings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define a website URL for scraping (example URL).\n",
    "website_url = \"https://example.com\"\n",
    "\n",
    "# Scrape the website to get a list of sentences.\n",
    "sentences = scrape_website(website_url)\n",
    "\n",
    "# Generate embeddings for the scraped sentences.\n",
    "sentence_embeddings = embeddings.embed_documents(sentences)\n",
    "\n",
    "# Insert the sentences and their embeddings into the Milvus collection with autogenerated IDs.\n",
    "collection.insert([list(range(len(sentences))), sentence_embeddings])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Section 12: Perform a Hybrid Search and Display Results**\n",
    "\n",
    "In this section, we will perform a hybrid search on the Milvus collection using a user-defined query. We will retrieve the most relevant documents based on this query and display the results. This process demonstrates the practical application of our previous configurations and data insertions.\n",
    "\n",
    "- **User Query Definition**: \n",
    "  - We start by defining a user query that we want to search for in the Milvus collection. In this example, the query is `\"What is this website about?\"`, which reflects the kind of information we wish to retrieve.\n",
    "\n",
    "- **Retrieving Relevant Documents**: \n",
    "  - Using the previously configured hybrid search retriever, we will obtain the most relevant documents that match the user query. The `get_relevant_documents(query)` method will leverage both dense and sparse embeddings to return results that are semantically and contextually relevant.\n",
    "\n",
    "- **Displaying Results**: \n",
    "  - We then iterate through the retrieved results and print the content of each document. This step allows us to view the actual sentences or text snippets associated with the query, providing insights into the information available in the Milvus collection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define a user query to search the collection.\n",
    "query = \"What is this website about?\"\n",
    "\n",
    "# Use the retriever to get the most relevant documents based on the query.\n",
    "results = retriever.get_relevant_documents(query)\n",
    "\n",
    "# Iterate through the retrieved results and print their content.\n",
    "for result in results:\n",
    "    print(result.page_content)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
